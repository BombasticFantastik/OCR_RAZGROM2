{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c587faa-d4eb-4f69-bdfe-2751b2ffb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch import tensor\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9c0d01-e43a-4404-87b2-75c4af3a20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Images_Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                path,\n",
    "                transforms=transforms.Compose([\n",
    "        transforms.Resize((256,256))\n",
    "        ,transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        ):\n",
    "        super(Images_Dataset,self).__init__()\n",
    "        self.data_path=path\n",
    "        self.image_transforms=transforms\n",
    "\n",
    "        self.all_data=[join(self.data_path,item) for item in listdir(self.data_path)]\n",
    "\n",
    "\n",
    "        json_path='vocab/vocab.json'\n",
    "        with open(json_path,'r') as file_option:\n",
    "            vocab=json.load(file_option)\n",
    "        self.let2int=vocab['let2int']\n",
    "        self.int2let=vocab['int2let']\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #with open(self.all_labels[idx],'r') as f: label=f.read().split(',')\n",
    "        return {\n",
    "            \"img\":self.image_transforms(Image.open(self.all_data[idx])),\n",
    "            'label':tensor([self.let2int[let] for let in self.all_data[idx].split('_')[0].replace('data/','')]),\n",
    "            'name':self.all_data[idx]\n",
    "        }\n",
    "        \n",
    "df=Images_Dataset('data')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a271e091-d74a-42e4-ba9f-6dfca7ecf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphabet=[symb for symb in 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя']\n",
    "\n",
    "let2int={let:i for i,let in enumerate(alphabet)}\n",
    "int2let={i:let for i,let in enumerate(alphabet)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e35a587-2667-471d-8c44-722afd780c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "class CRNN(Module):\n",
    "    def __init__(self,input_size,hidden_dim,num_classes):\n",
    "        super().__init__()\n",
    "        self.cnn_lay0=nn.Sequential(\n",
    "            nn.Conv2d(input_size,hidden_dim,3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.MaxPool2d((2,1),(2,1))\n",
    "        )\n",
    "        \n",
    "        self.cnn_lay1=nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim,hidden_dim*2,3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_dim*2),\n",
    "            nn.MaxPool2d((2,1),(2,1))\n",
    "        )\n",
    "                \n",
    "        self.cnn_lay2=nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*2,hidden_dim*4,3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_dim*4),\n",
    "            nn.MaxPool2d((2,1),(2,1))\n",
    "        )\n",
    "        self.cnn_lay3=nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*4,hidden_dim*8,3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_dim*8),\n",
    "            nn.MaxPool2d((2,1),(2,1))\n",
    "        )\n",
    "        self.cnn_lay4=nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*8,hidden_dim*16,3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(hidden_dim*16),\n",
    "            nn.MaxPool2d((2,1),(2,1))\n",
    "        )\n",
    "        self.rec_part=nn.LSTM(hidden_dim*16,hidden_dim*4,num_layers=1,bidirectional=True)\n",
    "        self.fin_lin=nn.Linear(hidden_dim*8,num_classes+1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        out0=self.cnn_lay0(x)\n",
    "\n",
    "        out1=self.cnn_lay1(out0)\n",
    "\n",
    "        out2=self.cnn_lay2(out1)\n",
    "\n",
    "\n",
    "        out3=self.cnn_lay3(out2)\n",
    "\n",
    "\n",
    "        out4=self.cnn_lay4(out3)#мб 2_0_1#попробовать батч фёрст\n",
    "        #print(out4.shape)\n",
    "        out4=out4.squeeze(2).permute(2,0,1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        rec_out,_=self.rec_part(out4)\n",
    " \n",
    "        pred=self.fin_lin(rec_out)\n",
    "        \n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7addcd45-9f79-419e-b274-ef58c702f0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 512])\n",
      "тъяизьрйлцдафвшхеныюч\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import PIL.Image\n",
    "from Dataset import Images_Dataset\n",
    "from Loop import train_loop\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import torch.nn as nn \n",
    "import os\n",
    "import torch\n",
    "import PIL\n",
    "import json\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "json_path='vocab/vocab.json'\n",
    "with open(json_path,'r') as file_option:\n",
    "    vocab=json.load(file_option)\n",
    "let2int=vocab['let2int']\n",
    "int2let=vocab['int2let']\n",
    "\n",
    "\n",
    "\n",
    "from Dataset import Images_Dataset\n",
    "from Model import CRNN\n",
    "from Loop import train_loop\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import torch.nn as nn \n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "option_path='config.yml'\n",
    "with open(option_path,'r') as file_option:\n",
    "    option=yaml.safe_load(file_option)\n",
    "\n",
    "device=option['device']\n",
    "trans=transforms=transforms.Compose([transforms.Resize((256,512)),transforms.ToTensor()])\n",
    "\n",
    "test_image=trans(PIL.Image.open('/home/artemybombastic/ArtemyBombasticGit/OCR_RAZGROM2/Test/ааамнчры_widock_bold.png'))\n",
    "#print(test_image.view(-1,3,32,128))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "model=CRNN(3,64,32).to(device)\n",
    "\n",
    "if f'weigts/model_weights.pth' in os.listdir('weights'):\n",
    "    weights_dict=torch.load(f'weights/model_weights.pth',weights_only=True)\n",
    "    model.load_state_dict(weights_dict)\n",
    "print(test_image.shape)\n",
    "out=model(test_image.view(-1,3,256,512).to(device))\n",
    "a=[int2let[str(i[0])] for i in out.argmax(2).tolist()]\n",
    "print(''.join(list(set(a))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06be5e8e-8feb-4e79-a22c-d20416fc0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import Images_Dataset\n",
    "from Model import CRNN\n",
    "from Loop import train_loop\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import torch.nn as nn \n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "option_path='config.yml'\n",
    "with open(option_path,'r') as file_option:\n",
    "    option=yaml.safe_load(file_option)\n",
    "\n",
    "device=option['device']\n",
    "\n",
    "\n",
    "\n",
    "train_dataset=Images_Dataset(option['path'])\n",
    "\n",
    "train_dataloader=DataLoader(dataset=train_dataset,batch_size=16,shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "model=CRNN(3,64,32).to(device)\n",
    "\n",
    "if f'model_weights.pth' in os.listdir('weights'):\n",
    "    weights_dict=torch.load(f'weights/model_weights.pth',weights_only=True)\n",
    "    model.load_state_dict(weights_dict)\n",
    "else:\n",
    "    print(1)\n",
    "\n",
    "\n",
    "\n",
    "optimizer=optim.Adam(model.parameters())\n",
    "loss_func=nn.CTCLoss()\n",
    "\n",
    "\n",
    "#train_loop(3,model=model,optimizer=detector_optimizer,loss_func=loss_func,dataloader=train_dataloader,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f27f3d80-d491-4424-9bcd-515c5897fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.639387607574463:  11%|████████████████▉                                                                                                                                      | 35/312 [00:09<01:12,  3.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     optimizer.step()\n\u001b[32m     16\u001b[39m     pbar.set_description(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_item\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweights/model_weights.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmean loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(losses)/\u001b[38;5;28mlen\u001b[39m(losses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/serialization.py:784\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    losses=[]\n",
    "    for batch in (pbar:=tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        pred=model(batch['img'].to(device))        \n",
    "        T = pred.size(0)\n",
    "        N = pred.size(1)\n",
    "        input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.int32)\n",
    "        target_lengths = torch.full(size=(N,), fill_value=8, dtype=torch.int32)\n",
    "        loss=loss_func(torch.log_softmax(pred,dim=2),batch['label'],input_lengths,target_lengths)\n",
    "        #loss=loss_func(pred,torch.ones((16,8)),input_lengths,target_lengths)\n",
    "        loss_item=loss.item()\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f'Loss: {loss_item}')\n",
    "        torch.save(model.state_dict(),'weights/model_weights.pth')\n",
    "    print(f'mean loss:{sum(losses)/len(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a6ca5eb-28d6-4bb7-9da2-2d9f3655eea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 33])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log_softmax(pred,dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03ea8a98-94da-4fa9-b393-f405d391a083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8cb1d-5ca7-4998-863a-ce2c20b145ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
